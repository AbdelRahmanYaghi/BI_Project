{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hPqxr2G8dT6-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob2 as glob\n",
        "import pandas as pd\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qIn-8DTndT7B"
      },
      "outputs": [],
      "source": [
        "files = glob.glob('16_8_model_weights_3_layers/*.npy')\n",
        "\n",
        "weights_dict = {'bias' : {}, 'weight' : {}}\n",
        "\n",
        "for file in files:\n",
        "    layer_name = file.split('/')[-1].split('.')[0]\n",
        "    if layer_name.split('_')[-1] == 'bias':\n",
        "        weights_dict['bias'].update({layer_name.split('_')[0]: np.load(file)})\n",
        "    else:\n",
        "        weights_dict['weight'].update({layer_name.split('_')[0]: np.load(file)})\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "test_df = pd.read_csv('Iris.csv').drop(columns = ['Id'])\n",
        "test_df['Species'] = test_df['Species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
        "\n",
        "test_y = test_df['Species']\n",
        "test_x = test_df.drop(columns = ['Species'])\n",
        "test_x = pd.DataFrame(scaler.fit_transform(test_x), columns = test_x.columns)\n",
        "\n",
        "order= ['fc1', 'fc2', 'fc3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "75k0BU0ZdT7C",
        "outputId": "2e533f08-c3f2-494c-ddc0-947816d44b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer Number\t|\t Weights Shape\t|\t Bias Shape\n",
            "------------------------------------------------------------\n",
            "Layer 1  \t|\t (16, 4) \t|\t (16,)\n",
            "Layer 2  \t|\t (8, 16) \t|\t (8,)\n",
            "Layer 3  \t|\t (3, 8) \t|\t (3,)\n"
          ]
        }
      ],
      "source": [
        "def choose_neurons_numeric(chosen_neurons, _weights_dict):\n",
        "\n",
        "    weights_dict_ = copy.deepcopy(_weights_dict)\n",
        "\n",
        "    weights_dict_['weight']['fc1'] = weights_dict_['weight']['fc1'][chosen_neurons[0], :]\n",
        "    weights_dict_['bias']['fc1'] = weights_dict_['bias']['fc1'][chosen_neurons[0]]\n",
        "\n",
        "    weights_dict_['weight']['fc2'] = weights_dict_['weight']['fc2'][chosen_neurons[1], :][:, chosen_neurons[0]]\n",
        "    weights_dict_['bias']['fc2'] = weights_dict_['bias']['fc2'][chosen_neurons[1]]\n",
        "\n",
        "    weights_dict_['weight']['fc3'] = weights_dict_['weight']['fc3'][: ,chosen_neurons[1]]\n",
        "\n",
        "    return weights_dict_\n",
        "\n",
        "def choose_neurons_matmul(chosen_neurons, _weights_dict):\n",
        "    weights_dict_ = copy.deepcopy(_weights_dict)\n",
        "\n",
        "    weights_dict_['weight']['fc1'] = chosen_neurons[0] * weights_dict_['weight']['fc1']\n",
        "\n",
        "    weights_dict_['weight']['fc2'] = chosen_neurons[1] * weights_dict_['weight']['fc2']\n",
        "    weights_dict_['weight']['fc2'] = weights_dict_['weight']['fc2'] * chosen_neurons[0].T\n",
        "\n",
        "    weights_dict_['weight']['fc3'] = weights_dict_['weight']['fc3'] * chosen_neurons[1].T\n",
        "\n",
        "    weights_dict_['bias']['fc1'] = chosen_neurons[0].flatten() * weights_dict_['bias']['fc1'].flatten()\n",
        "    weights_dict_['bias']['fc2'] = chosen_neurons[1].flatten() * weights_dict_['bias']['fc2'].flatten()\n",
        "\n",
        "    return weights_dict_\n",
        "\n",
        "\n",
        "def predict_numeric(img_df_row, weights_dict_):\n",
        "\n",
        "    x = np.dot(weights_dict_['weight']['fc1'], img_df_row.T) + weights_dict_['bias']['fc1']\n",
        "\n",
        "    x = np.dot(weights_dict_['weight']['fc2'], x) + weights_dict_['bias']['fc2']\n",
        "\n",
        "    x = np.dot(weights_dict_['weight']['fc3'], x) + weights_dict_['bias']['fc3']\n",
        "\n",
        "    return x\n",
        "\n",
        "def print_neurons_shape(weights_dict_):\n",
        "    print('Layer Number\\t|\\t Weights Shape\\t|\\t Bias Shape')\n",
        "    print('-'*60)\n",
        "    print('Layer 1  \\t|\\t', weights_dict_['weight']['fc1'].shape, '\\t|\\t' , weights_dict_['bias']['fc1'].shape)\n",
        "    print('Layer 2  \\t|\\t', weights_dict_['weight']['fc2'].shape, '\\t|\\t' , weights_dict_['bias']['fc2'].shape)\n",
        "    print('Layer 3  \\t|\\t', weights_dict_['weight']['fc3'].shape, '\\t|\\t' , weights_dict_['bias']['fc3'].shape)\n",
        "\n",
        "print_neurons_shape(weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "teWfFI8cdT7E",
        "outputId": "882716c4-56de-4c15-d5d4-2800a68a0082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9733333333333334\n",
            "0.06532579124930898\n"
          ]
        }
      ],
      "source": [
        "### Validation of results\n",
        "\n",
        "my_test_weights = choose_neurons_numeric([list(range(16)), list(range(8))], weights_dict)\n",
        "\n",
        "preds = np.array([predict_numeric(i, my_test_weights) for i in test_x.values])\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(preds)):\n",
        "    if preds[i].argmax() == test_y.iloc[i]:\n",
        "        counter += 1\n",
        "\n",
        "print(counter/150)\n",
        "\n",
        "def cross_entropy_loss(label, y_hat):\n",
        "\n",
        "    y = np.zeros_like(y_hat)\n",
        "    y[np.arange(label.shape[0]), label] = 1\n",
        "\n",
        "    exp_preds = np.exp(y_hat)\n",
        "\n",
        "    softmaxed_preds = exp_preds / exp_preds.sum(axis=1, keepdims=True)\n",
        "\n",
        "    return -np.sum(y * np.log(softmaxed_preds)) / len(test_y)\n",
        "\n",
        "\n",
        "def cross_entropy_loss_cplex(label, y_hat, model):\n",
        "\n",
        "    y = np.zeros_like(y_hat)\n",
        "    y[np.arange(label.shape[0]), label] = 1\n",
        "\n",
        "    exp_preds = np.e**(y_hat)\n",
        "\n",
        "    softmaxed_preds = exp_preds / exp_preds.sum(axis=1, keepdims=True)\n",
        "\n",
        "    to_sum = []\n",
        "\n",
        "    for u, i in zip(y, softmaxed_preds):\n",
        "      for o, j in zip(u, i):\n",
        "        to_sum.append(o * m.log(j))\n",
        "\n",
        "\n",
        "    return -np.sum(to_sum) / len(test_y)\n",
        "\n",
        "\n",
        "print(cross_entropy_loss(test_y, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApmryvAAdT7G"
      },
      "source": [
        "### DoCplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "D_YshTxwdod-"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install docplex cplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edFSYVUJdT7G",
        "outputId": "3646f385-a5e2-478b-cbd2-d1647903665b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from docplex.cp.model import CpoModel as Model\n",
        "def choose_and_move(_weights_dict_, log_output = None, neurons_to_remove = 1):\n",
        "    weights_dict_ = copy.deepcopy(_weights_dict_)\n",
        "    m = Model()\n",
        "\n",
        "    neurons_layer1 = len(weights_dict_['bias']['fc1'])\n",
        "    neurons_layer2 = len(weights_dict_['bias']['fc2'])\n",
        "\n",
        "    X_fc1 = np.array(m.binary_var_list(neurons_layer1, name='Layer 1 Neurons')).reshape((-1, 1))\n",
        "    X_fc2 = np.array(m.binary_var_list(neurons_layer2, name='Layer 2 Neurons')).reshape((-1, 1))\n",
        "\n",
        "    neurons_used = np.sum(X_fc1) + np.sum(X_fc2)\n",
        "\n",
        "    m.add_constraint(neurons_used == (neurons_layer1 + neurons_layer2 - neurons_to_remove))\n",
        "\n",
        "    new_weights_dict_ = choose_neurons_matmul([X_fc1, X_fc2], weights_dict_)\n",
        "\n",
        "    preds = np.array([predict_numeric(i, new_weights_dict_) for i in test_x.values])\n",
        "\n",
        "    loss = cross_entropy_loss_cplex(test_y, preds, m)\n",
        "\n",
        "    m.minimize(loss)\n",
        "\n",
        "    sol = m.solve(log_output=log_output)\n",
        "    \n",
        "    layer1 = np.ones(neurons_layer1)\n",
        "    layer2 = np.ones(neurons_layer2)\n",
        "\n",
        "    for i in sol.get_all_var_solutions():\n",
        "        _, layer_number, neuron_number = i.get_name().split(' ')\n",
        "\n",
        "        neuron_number_int = int(neuron_number.split('_')[-1])\n",
        "\n",
        "        if layer_number == '1':\n",
        "            layer1[neuron_number_int] = (i.get_value())\n",
        "\n",
        "        if layer_number == '2':\n",
        "            layer2[neuron_number_int] = (i.get_value())\n",
        "\n",
        "    print('==== Finished Iteration ====')\n",
        "    print('Loss:', sol.get_objective_values()[0])\n",
        "    print('Neurons Used:')\n",
        "    print(np.where(layer1 == 1)[0])\n",
        "    print(np.where(layer2 == 1)[0])\n",
        "\n",
        "    return layer1, layer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Hlc-h4dT7H",
        "outputId": "143012a4-687a-45b1-fad2-7a22e6c867e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Finished Iteration ====\n",
            "Loss: 0.06547558704562792\n",
            "Neurons Used:\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15]\n",
            "[0 1 2 3 4 5 6 7]\n",
            "==== Finished Iteration ====\n",
            "Loss: 0.0683419462607332\n",
            "Neurons Used:\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 13 14]\n",
            "[0 1 2 3 4 5 6 7]\n",
            "==== Finished Iteration ====\n",
            "Loss: 0.07218602119035404\n",
            "Neurons Used:\n",
            "[ 0  1  2  4  5  6  7  8  9 10 11 12 13]\n",
            "[0 1 2 3 4 5 6 7]\n",
            "==== Finished Iteration ====\n",
            "Loss: 0.07288669835025553\n",
            "Neurons Used:\n",
            "[ 0  1  2  3  5  6  7  8  9 10 11 12]\n",
            "[0 1 2 3 4 5 6 7]\n"
          ]
        }
      ],
      "source": [
        "layer1, layer2 = None, None\n",
        "weights_dict_copy_iterative = copy.deepcopy(weights_dict)\n",
        "for i in range(4):\n",
        "    layer1, layer2 = choose_and_move(weights_dict_copy_iterative)\n",
        "    weights_dict_copy_iterative = choose_neurons_numeric([np.where(layer1 == 1)[0], np.where(layer2 == 1)[0]], weights_dict_copy_iterative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "34TZYxwEdT7J",
        "outputId": "f7cc7540-ec36-46f5-e05d-3dcf2b692e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Finished Iteration ====\n",
            "Loss: 0.0687947261965805\n",
            "Neurons Used:\n",
            "[ 0  1  2  4  6  7  8  9 10 11 12 13 15]\n",
            "[0 1 2 3 4 5 6 7]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1.]))"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "choose_and_move(weights_dict, log_output = None, neurons_to_remove = 3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
